{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Leaf Classification Challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install pandas if you don't already have it (uncomment line below)\n",
    "# ! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you are working on Colab, data_utils can be downloaded using the command below (uncomment line below)\n",
    "# ! curl -O https://raw.githubusercontent.com/DeepLearningDTU/02456-2025/refs/heads/master/Week8%20Mini%20Project/data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "import data_utils\n",
    "\n",
    "#import sys\n",
    "#sys.path.append(os.path.join('.', '..')) # Allow us to import shared custom\n",
    "#                                         # libraries, like utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tying everything together\n",
    "\n",
    "Now that you have learned about the most common network architectures, it is time to combine them into a more advanced model. \n",
    "It often happens that you have a combination of data that cannot easily be modeled by any single one of these types of network. Knowing how to divide the data into the right subsets, and then build a network that handles each subset efficiently can mean the difference between a great model and an unusable one. \n",
    "\n",
    "In this notebook, we will work on the **Kaggle Leaf Classification Challenge**, a data science competition from [`kaggle.com`](https://www.kaggle.com/) that contains several different kinds of data.\n",
    "We will download the data, visualize it, and train a classifier.\n",
    "A simple network with poor performance is provided for you as a starting point, but it is up to you use what you have learnt to improve the results.\n",
    "\n",
    "\n",
    "## Kaggle challenge\n",
    "\n",
    "Kaggle is a website to participate in real-world challenges.\n",
    "Most competitions on Kaggle have a dataset, an accuracy metric and a leaderboard to compare submissions.\n",
    "You can read more about Kaggle public datasets [here](https://www.kaggle.com/datasets).\n",
    "\n",
    "We will undertake the [_Leaf Classification_](https://www.kaggle.com/c/leaf-classification) challenge. We report here the description of the dataset:\n",
    "\n",
    "> The dataset consists of approximately 1,584 images of leaf specimens which have been converted to binary black leaves against white backgrounds. \n",
    "Three sets of features are also provided per image: a shape contiguous descriptor, an interior texture histogram, and a ﬁne-scale margin histogram. For each feature, a 64-attribute vector is given per leaf sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get set up\n",
    "\n",
    "**NB**: You will need a Kaggle account for this exercise!\n",
    "\n",
    "1. Go to [Kaggle](https://www.kaggle.com/), create an account\n",
    "2. [Download the dataset](https://www.kaggle.com/c/leaf-classification/data)\n",
    "3. Unpack the dataset in the current directory. Structure should be as follows:\n",
    "```\n",
    "02456-deep-learning-with-PyTorch/6_Mini_Project/leaf-classification\n",
    "--> sample_submission.csv\n",
    "--> test.csv\n",
    "--> train.csv\n",
    "--> images\n",
    "--> --> 1.jpg\n",
    "--> --> 2.jpg\n",
    "--> --> 3.jpg\n",
    "--> --> ...\n",
    "```\n",
    "\n",
    "# Upload data to colab\n",
    "If you are running this notebook on Google Colab, you'll need to upload `data_utils.py` that we provide as well as the data you've just downloaded from Kaggle to Colab. Small files like `data_utils.py` you can simply upload via the folder icon to the left. Files that are uploaded like this, however, will be deleted every time the session ends. For big files like the kaggle data folder it's therefore better to:\n",
    "\n",
    "1. Upload the data to Google Drive\n",
    "2. Mount the Google Drive (see cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After going through a quick authorization process you should now have access to the uploaded file via colab. We can check the location of our data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls drive/'My Drive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the path for later use. Just replace this with your local path if you're not running on colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'drive/My Drive/leaf-classification/'\n",
    "path = 'leaf-classification/'  # if you're folder structure is as defined above and you want to run locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the data\n",
    "\n",
    "First we start out by looking at the images. \n",
    "You need to load them first!\n",
    "Then we load in the training data, which is in CSV format. For this, we use [pandas](https://pandas.pydata.org/).\n",
    "Pandas is useful for data analysis, but we don't suggest using it in any production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Observations:\t 1584\n"
     ]
    }
   ],
   "source": [
    "image_paths = glob.glob(path+'images/*.jpg')\n",
    "print(\"Total Observations:\\t\", len(image_paths))\n",
    "\n",
    "# now loading the train.csv to find features for each training point\n",
    "train = pd.read_csv(path + 'train.csv')\n",
    "train_images = [path+'images/{}.jpg'.format(i) for i in train.id.values]\n",
    "\n",
    "# now loading the test.csv\n",
    "test = pd.read_csv(path + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1.1) How many samples do we have for training and test? Do we have the same information for training and test data? How many samples do we have for each species?\n",
    "\n",
    "**Hint**: You might want to use .shape, .columns, pd.unique() and .symmetric_difference().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our training data and images loaded into memory.\n",
    "It is time to take a look at the data.\n",
    "Trying to classify leaves does not sound like a particularly difficult or interesting problem.\n",
    "We have probably all had teachers forcing us to do it on field trips as children.\n",
    "\n",
    "But try to take a look at **all** the different categories and come up with a system that discerns **all** types of leaves from each other. (In fact, distinguishing crops from weed using AI is already a thing: [weed-killing AI](https://www.cnbc.com/2018/06/04/weed-killing-ai-robot.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First we find an example of each species in order to visualize it\n",
    "species = np.array(sorted(train.species.unique()))\n",
    "species_examples = [np.random.choice(train[train.species == s].id.values) for s in species]\n",
    "\n",
    "# Then we gather its index in our list of images in order to find the correct image\n",
    "indexes = [image_paths.index(path + 'images/{}.jpg'.format(i)) for i in species_examples]\n",
    "\n",
    "# Display the first image\n",
    "plt.figure(figsize=(8, 8))\n",
    "image = imread(image_paths[indexes[0]], as_gray=True)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(\"%s\" % (species[0]))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "2.1) In general it is a good approach to visualize an image for each category to get a better feeling of the task. You should now write some code to show 1 image from each category. You might want to use plt.subplot()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot 1 image from each category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, classifying leaves is actually a very tough problem.\n",
    "What makes it even worse is that we cannot use all the image data we have available.\n",
    "In order to decrease the amount of computation needed, we need to reduce the size of the images as much as possible.\n",
    "On top of that, our neural network usually only accepts fixed-size input tensors.\n",
    "This means we will have to change the shape of the images so that they all have the same sizes.\n",
    "\n",
    "\n",
    "Resizing is problematic because it alters the shape of the leaves, and for some of them, this is their most distinctive feature. Take a look at `Salix_Intergra` in the bottom left corner.\n",
    "Describing this leaf without taking its shape into account seems extremely difficult.\n",
    "\n",
    "Therefore we will \n",
    "1. first pad all the images into squares, and\n",
    "2. then resize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "3.1) **Find an appropriate image size**. Test various resizings of the image until you have found the smallest resizing of the image where you \"can still differentiate between the images\".\n",
    "How small is too small should ultimately be determined by an actual test, but what makes sense visually is probably a good place to start.\n",
    "Change the `image_size = (?, ?)` parameter below, and note your choice.\n",
    " * **Answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3442408031.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[68], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    image_size = (?, ?)  # <-- YOUR CODE HERE\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Image pre-processing\n",
    "image_size = (?, ?)  # <-- YOUR CODE HERE\n",
    "\n",
    "# Amount of images\n",
    "amount = 5\n",
    "image_sample = np.random.choice(train_images, amount)\n",
    "\n",
    "# Define figure size\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Save original images in the figure\n",
    "ax = plt.subplot(2, amount + 1, 1)\n",
    "txt = ax.text(0.4, 0.5, 'Original', fontsize=20)\n",
    "txt.set_clip_on(False)\n",
    "plt.axis('off')\n",
    "for i, path in enumerate(image_sample):\n",
    "    plt.subplot(2, amount + 1, i + 2)\n",
    "    image = imread(path, as_gray=True)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    _id = int(path.split('/')[-1].split('.')[0])\n",
    "    plt.title(\"{0}\\nshape: {1}\".format(train[train.id == _id].species.values[0], image.shape))\n",
    "    plt.axis('off')\n",
    "\n",
    "# Save resized images in the figure\n",
    "ax = plt.subplot(2, amount + 1, len(image_sample) + 2)\n",
    "txt = ax.text(0.4, 0.5, 'Resized', fontsize=20)\n",
    "txt.set_clip_on(False)\n",
    "plt.axis('off')\n",
    "for i, path in enumerate(image_sample):\n",
    "    i += len(image_sample) + 3\n",
    "    plt.subplot(2, amount + 1, i)\n",
    "    image = imread(path, as_gray=True)\n",
    "    image = data_utils.pad2square(image)  # Make the image square\n",
    "    image = resize(image, output_shape=image_size, mode='reflect', anti_aliasing=True)  # resizes the image\n",
    "    plt.imshow(image, cmap='gray')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the other features\n",
    "\n",
    "Now that we have looked at the image data we have available, it is time to take a look at the other available features. Below we choose a random subset of the training data, and visualize the 3 types of available features:\n",
    "* margin\n",
    "* shape\n",
    "* texture\n",
    "\n",
    "Run it a few times to try and get an understanding of how the features differ from species to species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "X = train.values\n",
    "species = X[:, 1:2]\n",
    "margin = X[:, 2:66]\n",
    "shape = X[:, 66:130]\n",
    "texture = X[:, 130:]\n",
    "\n",
    "# Let us plot some of the features\n",
    "plt.figure(figsize=(21,7)) # Set the plot size\n",
    "amount = 5                 # Choose the amount of images we want to show at a time\n",
    "\n",
    "for i, idx in enumerate(np.random.choice(range(len(train)), amount)):\n",
    "    ax = plt.subplot(amount,4,1+i*4)\n",
    "    txt = ax.text(0.2, 0.2, species[idx][0], fontsize=20)\n",
    "    txt.set_clip_on(False)\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title('Species', fontsize=20)\n",
    "    plt.subplot(amount,4,2+i*4)\n",
    "    plt.plot(margin[idx])\n",
    "    if i == 0:\n",
    "        plt.title('Margin', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(amount,4,3+i*4)\n",
    "    plt.plot(shape[idx])\n",
    "    if i == 0:\n",
    "        plt.title('Shape', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(amount,4,4+i*4)\n",
    "    plt.plot(texture[idx])\n",
    "    if i == 0:\n",
    "        plt.title('Texture', fontsize=20)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "So far we have learned about feed forward neural networks (FFNN), convolutional neural networks (CNN), recurrent neural networks (RNN), and transformers.\n",
    "\n",
    "4.1) How could the `image`, `Margin`, `Shape` and `Texture` be used for classification, i.e. what kind of network type would you use for each of them, and why?\n",
    " * **Answer:**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing the data\n",
    "\n",
    "The details of the code in this section isn't that important.\n",
    "It simply manages the data in a nice way - so it is a good place to come back and look for inspiration when you will work on your own projects.\n",
    "\n",
    "\n",
    "## Defining the data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 \n",
    "5.1) As a part of the data loader, we should specify the shape of the images, number of classes, and the number of features for the three feature types: margin, shape and texture. Define the three variables in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image shape should be of the format (height, width, channels)\n",
    "IMAGE_SHAPE =    # <-- Your answer here\n",
    "NUM_CLASSES =    # <-- Your answer here\n",
    "\n",
    "# For all three features types margin, shape, and texture, we have NUM_FEATURES for each type.\n",
    "NUM_FEATURES =   # <-- Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "TRAIN_PATH = path + \"train.csv\"\n",
    "TEST_PATH = path + \"test.csv\"\n",
    "IMAGE_PATHS = glob.glob(path + \"images/*.jpg\")\n",
    "\n",
    "# train holds both X (input) and t (target/truth)\n",
    "data = data_utils.load_data(train_path=TRAIN_PATH,\n",
    "                            test_path=TEST_PATH,\n",
    "                            image_paths=IMAGE_PATHS,\n",
    "                            image_shape=IMAGE_SHAPE[:2])\n",
    "# to visualize the size of the dimensions of the data\n",
    "print(\"\\n@@@Shape checking of data sets@@@\")\n",
    "print(\"TRAIN\")\n",
    "print(\"\\timages\\t%s%f\" % (data.train['images'].shape, data.train['images'].mean()))\n",
    "print(\"\\tmargins\\t%s\\t%f\" % (data.train['margins'].shape, data.train['margins'].mean()))\n",
    "print(\"\\tshapes\\t%s\\t%f\" % (data.train['shapes'].shape, data.train['shapes'].mean()))\n",
    "print(\"\\ttextures%s\\t%f\" % (data.train['textures'].shape, data.train['textures'].mean()))\n",
    "print(\"\\tts\\t %s\" % (data.train['ts'].shape))\n",
    "print(\"\\twhile training, batch_generator will onehot encode ts to (batch_size, num_classes)\")\n",
    "print(\"TEST\")\n",
    "print(\"\\timages\\t%s\\t%f\" % (data.test['images'].shape, data.test['images'].mean()))\n",
    "print(\"\\tmargins\\t%s\\t%f\" % (data.test['margins'].shape, data.test['margins'].mean()))\n",
    "print(\"\\tshapes\\t%s\\t%f\" % (data.test['shapes'].shape, data.test['shapes'].mean()))\n",
    "print(\"\\ttextures%s\\t%f\" % (data.test['textures'].shape, data.test['textures'].mean()))\n",
    "print(\"\\tids\\t%s\" % (data.test['ids'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do you want to load your data quickly next time?\n",
    "In the above cell the data is loaded, reshaped and stored as an object. If you want load this object quickly next time you should go for pickle. Pickle does effectively write an object into a character stream, so it can be loaded fastly next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Dump the data into a pickle file\n",
    "# with open(path + '/data.pickle', 'wb') as f:\n",
    "#     pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the data from a pickle file\n",
    "# with open(path + '/data.pickle', 'rb') as f:\n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator\n",
    "\n",
    "While training, we will not directly access the entire dataset, instead we have a `batch_generator` function to give us inputs aligned with their targets/ids in a size that our model can handle in memory (batch\\_size).\n",
    "\n",
    "Furthermore, the `batch_generator` also handles validation splitting.\n",
    "\n",
    "## Exercise 6\n",
    "6.1) Explain shortly why the size of batches is important. You should comment on how the size of batches affect the memory, training speed, and the estimates of the gradients.\n",
    "Choose a reasonable batch size the `batch_size = ?` parameter below, and note your choice.\n",
    "\n",
    " * **Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = ?\n",
    "dummy_batch_gen = data_utils.batch_generator(data, batch_size=batch_size, num_classes=NUM_CLASSES, num_iterations=5e3, seed=42)\n",
    "train_batch = next(dummy_batch_gen.gen_train())\n",
    "valid_batch, i = next(dummy_batch_gen.gen_valid())\n",
    "test_batch, i = next(dummy_batch_gen.gen_test())\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(\"\\timages,\", train_batch['images'].shape)\n",
    "print(\"\\tmargins,\", train_batch['margins'].shape)\n",
    "print(\"\\tshapes,\", train_batch['shapes'].shape)\n",
    "print(\"\\ttextures,\", train_batch['textures'].shape)\n",
    "print(\"\\tts,\", train_batch['ts'].shape)\n",
    "print()\n",
    "print(\"VALID\")\n",
    "print(\"\\timages,\", valid_batch['images'].shape)\n",
    "print(\"\\tmargins,\", valid_batch['margins'].shape)\n",
    "print(\"\\tshapes,\", valid_batch['shapes'].shape)\n",
    "print(\"\\ttextures,\", valid_batch['textures'].shape)\n",
    "print(\"\\tts,\", valid_batch['ts'].shape)\n",
    "print()\n",
    "print(\"TEST\")\n",
    "print(\"\\timages,\", test_batch['images'].shape)\n",
    "print(\"\\tmargins,\", test_batch['margins'].shape)\n",
    "print(\"\\tshapes,\", test_batch['shapes'].shape)\n",
    "print(\"\\ttextures,\", test_batch['textures'].shape)\n",
    "print(\"\\tids,\", len(test_batch['ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "7.1)  Now you must define the network architecture. It is always a good idea to start simple. We recommend you to start with a mix of a convolutional layer (maybe followed by max pooling layer), a recurrent layer, and a linear output layer such that we use all the features.  \n",
    "\n",
    "When you build the model you should be aware of the dimensions of the input and output for your different layers. The function`permute` and `view` will be very useful to rearrange your dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = IMAGE_SHAPE\n",
    "\n",
    "conv_out_channels =  # <-- Filters in your convolutional layer\n",
    "kernel_size =        # <-- Kernel size\n",
    "conv_stride =        # <-- Stride\n",
    "conv_pad    =        # <-- Padding\n",
    "\n",
    "# Keep track of features to output layer\n",
    "features_cat_size = # <-- Number of features concatenated before output layer\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_1 = Conv2d(in_channels=channels,\n",
    "                             out_channels=conv_out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=conv_stride,\n",
    "                             padding=conv_pad)\n",
    "\n",
    "        # Exercise: Add a recurrent unit like and RNN or GRU\n",
    "        # >> YOUR CODE HERE <<\n",
    "        self.rnn_1 =\n",
    "\n",
    "        self.l_out = Linear(in_features=features_cat_size,\n",
    "                            out_features=NUM_CLASSES,\n",
    "                            bias=False)\n",
    "\n",
    "    def forward(self, x_img, x_margin, x_shape, x_texture):\n",
    "        features = []\n",
    "        out = {}\n",
    "\n",
    "        ## Convolutional layer ##\n",
    "        # - Change dimensions to fit the convolutional layer\n",
    "        # - Apply Conv2d\n",
    "        # - Use an activation function\n",
    "        # - Change dimensions s.t. the features can be used in the final FFNN output layer\n",
    "\n",
    "        # >> YOUR CODE HERE <<\n",
    "\n",
    "        # Append features to the list \"features\"\n",
    "        features.append(features_img)\n",
    "\n",
    "\n",
    "        ## Use concatenated leaf features for FFNN ##\n",
    "        x = torch.cat((x_margin, x_texture), dim=1)  # if you want to use features as feature vectors\n",
    "        features_vector = x\n",
    "        features.append(features_vector)\n",
    "\n",
    "\n",
    "        ## Use concatenated leaf features for RNN ##\n",
    "        # - Chage dimensions to fit GRU\n",
    "        # - Apply GRU\n",
    "        # - Change dimensions s.t. the features can be used in the final FFNN output layer\n",
    "\n",
    "        # >> YOUR CODE HERE <<\n",
    "\n",
    "        # Append features to the list \"features\"\n",
    "        features.append(features_rnn)\n",
    "\n",
    "\n",
    "        ## Output layer where all features are in use ##\n",
    "        features_final = torch.cat(features, dim=1)\n",
    "\n",
    "        out['out'] = self.l_out(features_final)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "8.1) Since this is a classification task we will use the cross-entropy loss. Define the cross-entropy loss as the loss function in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "criterion =           # <-- Your code here.\n",
    "\n",
    "# weight_decay is equal to L2 regularization\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test network\n",
    "\n",
    "#### Debugging \n",
    "The following cell might cause you some errors - try these suggestions before you try anyting else.\n",
    "\n",
    "* **Your kernel keeps dying** on the line below it is most likely because you run out of memory.\n",
    "The two most likely solutions are \n",
    " 1. reduce the image size further\n",
    " 1. change your network architecture such that it uses less resources\n",
    "\n",
    "* **`RuntimeError: size mismatch, m1: [??? x ???], m2: [??? x ???]`** \n",
    " 1. `features_cat_size` must match the actual output of the network i.e. the second dimension in `m1`.\n",
    "\n",
    "* **Training is very slow**. This is most likely caused by the images. \n",
    " 1. Try and reduce the size of the images further, or reduce the dimensions of the network using either pooling or strides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_img_shape = tuple([batch_size] + list(IMAGE_SHAPE))\n",
    "_feature_shape = (batch_size, NUM_FEATURES)\n",
    "\n",
    "def randnorm(size):\n",
    "    return np.random.normal(0, 1, size).astype('float32')\n",
    "\n",
    "# dummy data\n",
    "_x_image = get_variable(Variable(torch.from_numpy(randnorm(_img_shape))))\n",
    "_x_margin = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "_x_shape = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "_x_texture = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "\n",
    "# test the forward pass\n",
    "output = net(x_img=_x_image, x_margin=_x_margin, x_shape=_x_shape, x_texture=_x_texture)\n",
    "output['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup settings for training\n",
    "VALIDATION_SIZE = 0.1 # 0.1 is ~ 100 samples for validation\n",
    "max_iter = 1000\n",
    "log_every = 100\n",
    "eval_every = 100\n",
    "\n",
    "# Function to get label\n",
    "def get_labels(batch):\n",
    "    return get_variable(Variable(torch.from_numpy(batch['ts']).long()))\n",
    "\n",
    "# Function to get input\n",
    "def get_input(batch):\n",
    "    return {\n",
    "        'x_img': get_variable(Variable(torch.from_numpy(batch['images']))),\n",
    "        'x_margin': get_variable(Variable(torch.from_numpy(batch['margins']))),\n",
    "        'x_shape': get_variable(Variable(torch.from_numpy(batch['shapes']))),\n",
    "        'x_texture': get_variable(Variable(torch.from_numpy(batch['textures'])))\n",
    "    }\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "# Generate batches\n",
    "batch_gen = data_utils.batch_generator(data,\n",
    "                                       batch_size=batch_size,\n",
    "                                       num_classes=NUM_CLASSES,\n",
    "                                       num_iterations=max_iter,\n",
    "                                       seed=42,\n",
    "                                       val_size=VALIDATION_SIZE)\n",
    "\n",
    "# Train network\n",
    "net.train()\n",
    "for i, batch_train in enumerate(batch_gen.gen_train()):\n",
    "    if i % eval_every == 0:\n",
    "\n",
    "        # Do the validaiton\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "        for batch_valid, num in batch_gen.gen_valid():\n",
    "            output = net(**get_input(batch_valid))\n",
    "            labels_argmax = torch.max(get_labels(batch_valid), 1)[1]\n",
    "            val_losses += criterion(output['out'], labels_argmax) * num\n",
    "            val_accs += accuracy(output['out'], labels_argmax) * num\n",
    "            val_lengths += num\n",
    "\n",
    "        # Divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        valid_loss.append(get_numpy(val_losses))\n",
    "        valid_accs.append(get_numpy(val_accs))\n",
    "        valid_iter.append(i)\n",
    "#         print(\"Valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, valid_loss[-1], valid_accs[-1]))\n",
    "        net.train()\n",
    "\n",
    "    # Train network\n",
    "    output = net(**get_input(batch_train))\n",
    "    labels_argmax = torch.max(get_labels(batch_train), 1)[1]\n",
    "    batch_loss = criterion(output['out'], labels_argmax)\n",
    "\n",
    "    train_iter.append(i)\n",
    "    train_loss.append(float(get_numpy(batch_loss)))\n",
    "    train_accs.append(float(get_numpy(accuracy(output['out'], labels_argmax))))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log i figure\n",
    "    if i % log_every == 0:\n",
    "        fig = plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_iter, train_loss, label='train_loss')\n",
    "        plt.plot(valid_iter, valid_loss, label='valid_loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_iter, train_accs, label='train_accs')\n",
    "        plt.plot(valid_iter, valid_accs, label='valid_accs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        clear_output(wait=True)\n",
    "#         print(\"Train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, train_loss[-1], train_accs[-1]))\n",
    "\n",
    "    if max_iter < i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "**Tip** This is a very small dataset (number of observations) compared to the number of features.\n",
    "This means that overfitting may be an issue, and sometimes fancy tricks won't do any good. \n",
    "Keep that in mind, and always start simple.\n",
    "\n",
    "**9.1) Improve the network**, and get as high a validation score as you can. \n",
    "When trying to improve the network nothing is sacred. You can try various learning rates, batch sizes, validation sizes, etc. \n",
    "And most importantly, the validation set is very small (only 1 sample per class), etc.\n",
    "\n",
    "To get you off to a good start we have created a list of **things you might want to try**:\n",
    "* Add more layers (mostly fully connected and convolutional)\n",
    "* Increase or decrease the batch size \n",
    "* Use dropout (a lot - e.g. between the convolutional layers)\n",
    "* Use batch normalization (a lot)\n",
    "* Try with L2 regularization (weight decay)\n",
    "* Use only the image for training (with CNN) - comment on the increased time between iterations.\n",
    "* Change the image size to be bigger or smaller\n",
    "* Try other combinations of FFN, CNN, RNN parts in various ways (bigger is not always better)\n",
    "\n",
    "If your network is not performing as well as you would like it to, [here](http://theorangeduck.com/page/neural-network-not-working) is a great explanation of what might have gone wrong.\n",
    "\n",
    "\n",
    "**9.2) Improve Kaggle score**. Once happy try to get the best score on Kaggle for this dataset as you can (**upload** instructions below)\n",
    "You can upload your solution multiple times as you progress.\n",
    "A very good implementation would get a score between $0.04$ to $0.06$ (the smaller the better), try and see if you can get there, and explain what might have gone wrong if you can't. \n",
    "\n",
    "\n",
    "**9.3) Reflect on the process**, and how you got to your final design and discuss your final results. \n",
    "What worked, and what didn't?\n",
    "Include at least the following: \n",
    "* Description of the final architecture\n",
    "* Description of the training parameters\n",
    "* Description of the final results (Kaggle and validation)\n",
    "\n",
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission to Kaggle\n",
    "\n",
    "First we have to make test set predictions, then we have to place the output in the submission file and then upload to Kaggle to get our score! You can upload up to 5 submissions per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET PREDICTIONS\n",
    "# containers to collect ids and predictions\n",
    "ids_test, preds_test = [], []\n",
    "net.eval()\n",
    "# run like with validation\n",
    "for batch_test, num in batch_gen.gen_test():\n",
    "    output = net(**get_input(batch_test))\n",
    "    y_out = output['out'].data\n",
    "\n",
    "    ids_test += batch_test['ids']\n",
    "    if num!=len(y_out):\n",
    "        # in case of the last batch, num will be less than batch_size\n",
    "        y_out = y_out[:num]\n",
    "    preds_test.append(y_out)\n",
    "preds_test = np.concatenate(preds_test, axis=0)\n",
    "assert len(ids_test) == len(preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds_test, columns=data.le.classes_)\n",
    "ids_test_df = pd.DataFrame(ids_test, columns=[\"id\"])\n",
    "submission = pd.concat([ids_test_df, preds_df], axis=1)\n",
    "submission.to_csv(drive_path + 'submission.csv', index=False)\n",
    "\n",
    "# below prints the submission, can be removed and replaced with code block below\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload submission\n",
    "\n",
    "1. Go to [`https://www.kaggle.com/c/leaf-classification/submit`](https://www.kaggle.com/c/leaf-classification/submit)\n",
    "3. Click or drop your submission here (writing a description is good practice)\n",
    "4. Submit and look at where you are on the leaderboard.\n",
    "\n",
    "Success! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
